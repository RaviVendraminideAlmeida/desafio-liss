{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104860cb-034b-49eb-9368-16e3c44cd415",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install selenium \n",
    "! pip install undetected-chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a458aa9-4df1-4392-a125-3d9ab618c137",
   "metadata": {},
   "source": [
    "### Metodologia\n",
    "Há diversas formas de realizar a coleta de dados de um site. Tal ato é chamado de Data Scraping/Web Crawling. Para isso são utilizadas bibliotecas que forneçam ferramentas e drivers para controle de Navegadores.\n",
    "As principais abordagens que se mostraram viáveis ao analizar como funciona a arquitetura do alvo em questão (JusBrasil) foram:\n",
    "1.  Navegar pelas páginas e pegar os dados disponibilizados diretamente no HTML\n",
    "    > Essa abordagem tem a vantagem de ser de implementação relativamente simples\n",
    "2.  Conseguir os dados a partir da interceptação das requisções e respostas realizadas pela páginas vindas da URL \"https://www.jusbrasil.com.br/polaris-processos/graphql\"\n",
    "       > Essa abordagem tem a vantagem dos dados serem serializados em JSON pelo Back-End da plataforma e, portanto esterão normalizados e serão fáceis de utilizar. Por outro lado, possui uma implementação mais complicada\n",
    "\n",
    "Eu escolhi utilizar a primeira abordagem pois a biblioteca Python utilizada para interceptar as requisições do browser (SeleniumWire) é detectada pela tecnologia anti-bots da CloudFlare (CDN Responsável por servir as páginas estáticas). Com um pouco de pesquisa percebi que a biblioteca Pupeteer (NodeJS) possui um suporte muito bom para essa técnica, no entanto, por conta da minha maior familiaridade com python escolhi a primeira alternativa\n",
    "\n",
    "### Ferramentas\n",
    "#### Linguagens de Programação\n",
    "* Python - Por conta alta produtividade fornecida pela linguagem e das bibliotecas de automação com grandes comunidades\n",
    "\n",
    "#### Bibliotecas\n",
    "* Selenium - Uma biblioteca utilizada para automação web, permite selecionar e manipular elementos HTML\n",
    "* Undetected Chrome - Uma distribuição de um navegador baseado em Chromium que possui menos rastros da automação e por isso não pode ser detectada pela tecnologia anit-bots da CloudFlare)\n",
    "* CSV - Biblioteca nativa do python utilizada para armazenar registros em um arquivo CSV\n",
    "* Pandocs - Utilizado para gerar a documentação a partir do arquivo com extensão .ipynb (Python Notebooks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72936a7-4bd5-4d4e-a340-6da5f0aac8c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import undetected_chromedriver as uc\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ff7ff-74af-4371-b365-d4e815b36684",
   "metadata": {},
   "source": [
    "### Conseguindo os links das empresas\n",
    "\n",
    "#### Conseguindo os links\n",
    "Dentro da plataforma do JusBrasil uma empresa pode ser referenciada com diversos nomes variando coisas como pontuação, caracteres maísculos e minúsculos, etc.\n",
    "Para conseguir os links que listam os processos com todos esses nomes essa função recebe o link que leva à página de pesquisa de uma empresa e a acessa. Depois disso são selecionados todos os elementos de navegação (tag <a></a>) que levam às páginas dos processos. Pegamos as propriedades que contém os links para os quais esses elementos iriam redirecionar e armazenamos em um vetor que será retornado ao final da função\n",
    "\n",
    "#### Paginação\n",
    "O principal desafio ao escrever o código dessa função é lidar com a forma como os links são separados por páginas, felizmente é possível avançar na páginação apenas adicionando o parâmetro \"&p=numero_da_pagina\" à URL, para isso foi necessário utilizar um laço de repetição ENQUANTO, que a cada iteração aumentara em 1 o número, se na página não houver nenhum link para uma página de empresa o laço será interrompido e a lista com as urls obtidas será retornada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9e302b-2afb-4ac2-a526-c58786129fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conseguir_links_empresa(empresa, driver):\n",
    "\n",
    "    i = 1\n",
    "    \n",
    "    LINKS_FINAL = []\n",
    "     \n",
    "    while True:       \n",
    "        driver.get(f\"{empresa}&p={i}\")\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        \n",
    "        elems = driver.find_elements(By.XPATH, '//a[@class=\"EntitySnippet-anchor\"]')\n",
    "       \n",
    "        links = [elem.get_attribute('href') for elem in elems]\n",
    "\n",
    "        if len(links) == 0:\n",
    "            break\n",
    "        else:\n",
    "            for link in links:\n",
    "                LINKS_FINAL.append(link)\n",
    "            i += 1    \n",
    "               \n",
    "    return LINKS_FINAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff36ee48-8e72-46d6-93ce-73e21ce1b106",
   "metadata": {},
   "source": [
    "### Conseguindo as informações dos processos\n",
    "\n",
    "A função recebe os links das empresas (gerado pela função anterior) e acessa cada um deles pegando as informações dos processos disponíveis nas páginas, as compilando em um dicionário e adicionando esse dicionário a uma lista.\n",
    "\n",
    "#### Carregamento assíncrono das informações\n",
    "O principal desafio ao recolher as informações dessa página é o carregamento assíncrono na páginas, ou seja, os elementos são carregados conforme a página é rolada para baixo. A minha solução para esse problema é pegar o contador de processos no início da página rolar a página para baixo utilizando Selenium até que o número de elementos presente na página deixe de ser menor que o número indicado no contador. Depois disso todos os dados podem ser compilados em dicionários e adicionados à lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06900258-7e29-42f2-ab1f-89bd1cbfc397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conseguir_informacoes_processos(links_empresa, driver):   \n",
    "    \n",
    "    lista_info_processos = []\n",
    "    \n",
    "    for link_empresa in links_empresa:\n",
    "\n",
    "        driver.get(link_empresa)\n",
    "\n",
    "        elementos_contador = driver.find_elements(By.XPATH, '//strong[@class=\"LawsuitCounter\"]/span')\n",
    "        \n",
    "        elems = []\n",
    "        \n",
    "        if len(elementos_contador) > 0:    \n",
    "\n",
    "            contador = int(elementos_contador[0].get_attribute('innerText').replace(\".\", \"\"))\n",
    "\n",
    "            if contador > 1000:\n",
    "                contador = 1000\n",
    "            \n",
    "            elems = driver.find_elements(By.CSS_SELECTOR, 'LawsuitList-item')\n",
    "            \n",
    "            while len(elems) < contador:\n",
    "\n",
    "                print(f\"Contador: {contador} | Elementos: {len(elems)}\")\n",
    "                \n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                elems = driver.find_elements(By.CSS_SELECTOR, '.LawsuitList-item')\n",
    "\n",
    "                driver.implicitly_wait(2)\n",
    "\n",
    "\n",
    "        for elem in elems:\n",
    "\n",
    "            try:\n",
    "                link = \"Indisponível\"\n",
    "    \n",
    "                if len(elem.find_elements(By.CSS_SELECTOR, '.LawsuitCardPersonPage-title--link')) > 0:\n",
    "                    if elem.find_element(By.CSS_SELECTOR, '.LawsuitCardPersonPage-title--link').get_attribute('href') != None:\n",
    "                        link = elem.find_element(By.CSS_SELECTOR, '.LawsuitCardPersonPage-title--link').get_attribute('href')\n",
    "                \n",
    "                numero_processo = elem.find_element(By.CSS_SELECTOR, '.LawsuitCardPersonPage-title--link .LawsuitCardPersonPage-header-processNumber').get_attribute('innerText')          \n",
    "                \n",
    "                tribunal_localidade = \"Indisponível\"\n",
    "                \n",
    "                if len(elem.find_elements(By.CSS_SELECTOR, 'p.LawsuitCardPersonPage-body-row-item-text[role=\"body-court\"]')) > 0:\n",
    "                    tribunal_localidade = elem.find_element(By.CSS_SELECTOR, 'p.LawsuitCardPersonPage-body-row-item-text[role=\"body-court\"]').get_attribute('innerText')\n",
    "    \n",
    "                if len(elem.find_elements(By.CSS_SELECTOR, 'p.LawsuitCardPersonPage-body-row-item-text[role=\"body-kind\"]')) > 0:\n",
    "                    procedimento = elem.find_elements(By.CSS_SELECTOR, 'p.LawsuitCardPersonPage-body-row-item-text[role=\"body-kind\"]')[0].get_attribute('innerText')\n",
    "                else:\n",
    "                    procedimento = \"\"\n",
    "                \n",
    "                partes = elem.find_element(By.CSS_SELECTOR, 'strong.LawsuitCardPersonPage-header-processInvolved').get_attribute('innerText')\n",
    "                # número do processo, tribunal, localidade, UF, classe ou procedimento e partes envolvidas.\n",
    "    \n",
    "                localidade = 'Indisponível'\n",
    "                tribunal = 'Indisponível'\n",
    "                \n",
    "                if '·' in  tribunal_localidade:\n",
    "                    tribunal = tribunal_localidade.split('·')[0]\n",
    "                    localidade = tribunal_localidade.split('·')[1]\n",
    "                else:\n",
    "                    tribunal = tribunal_localidade\n",
    "    \n",
    "                uf = \"\"\n",
    "    \n",
    "                if tribunal != \"Indisponível\":\n",
    "                    uf = tribunal.strip()[-2:]\n",
    "                \n",
    "                processo = {\n",
    "                    \"url\": link,\n",
    "                    \"numero_processo\" : numero_processo,\n",
    "                    \"tribunal\" : tribunal,\n",
    "                    \"localidade\" : localidade,\n",
    "                    \"procedimento\" : procedimento,\n",
    "                    \"tribunal\" : tribunal,\n",
    "                    \"uf\" : uf,\n",
    "                    \"partes\" : partes.split('x') if partes != None else \"Indisponível\"\n",
    "                }\n",
    "    \n",
    "                print(processo)\n",
    "                \n",
    "                lista_info_processos.append(processo)\n",
    "\n",
    "            except Exception:\n",
    "                print(f\"Erro: {Exception}\")\n",
    "                pass\n",
    "            \n",
    "            if len(lista_info_processos) >= 1000:\n",
    "                return lista_info_processos\n",
    "    \n",
    "    return lista_info_processos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beff352-ba4a-4b44-8ba3-e5230bcb8cf5",
   "metadata": {},
   "source": [
    "### Salvando o resultado das consultas em arquivos\n",
    "\n",
    "Essa função recebe o nome do arquivo a ser salvo e a lista de dicionários que será transformada num arquivo CSV, depois disso são pegas as chaves do dicionário e colocadas na primeira linha do arquivo, então são pegos os registros e dispostos em linhas correspondentes às suas respectivas colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6107fcf-c401-42a0-b0b5-cdb33e3b625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_arquivo_csv(nome_arquivo, lista):\n",
    "\n",
    "    chaves = lista[0].keys()\n",
    "    \n",
    "    with open(f'{nome_arquivo}', 'w', newline='') as arquivo_saida:\n",
    "        dict_writer = csv.DictWriter(arquivo_saida, chaves)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(lista)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf7b7fa-d39b-47ab-b873-281e3a8ca508",
   "metadata": {},
   "source": [
    "### Efetivamente realizando o processo de Scraping\n",
    "\n",
    "Para realizar o processo de Scraping basta instanciar o driver do navegador, chamar a função que busca os links, passar seu resultado para a função que busca as informações e depois passar esse resultado para a função que salva os arquivos, dessa forma é possível compilar as informações disponíveis na plataforma em um arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f64c7c-39e2-4260-b7f1-e1e8cd04f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = uc.Chrome(headless=False,use_subprocess=False)\n",
    "\n",
    "links_empresa_puma = conseguir_links_empresa('https://www.jusbrasil.com.br/consulta-processual/busca?q=Puma+do+Brasil+Ltda.', driver)\n",
    "info_processos_puma = conseguir_informacoes_processos(links_empresa_puma, driver)\n",
    "salvar_arquivo_csv('ds_puma.csv', info_processos_puma)\n",
    "\n",
    "links_empresa_adidas = conseguir_links_empresa('https://www.jusbrasil.com.br/consulta-processual/busca?q=Adidas+do+Brasil+Ltda.', driver)\n",
    "info_processos_adidas = conseguir_informacoes_processos(links_empresa_adidas, driver)\n",
    "salvar_arquivo_csv('ds_adidas.csv', info_processos_adidas)\n",
    "\n",
    "links_empresa_nike = conseguir_links_empresa('https://www.jusbrasil.com.br/consulta-processual/busca?q=Nike+do+Brasil+Comercio+e+Participações+Ltda', driver)\n",
    "info_processos_nike = conseguir_informacoes_processos(links_empresa_nike, driver)\n",
    "salvar_arquivo_csv('ds_nike.csv', info_processos_nike)\n",
    "\n",
    "links_empresa_asics = conseguir_links_empresa('https://www.jusbrasil.com.br/consulta-processual/busca?q=Asics+Brasil%2C+Distribui%C3%A7%C3%A3o+e+Com%C3%A9rcio+de+Artigos+Esportivos+Ltda', driver)\n",
    "info_processos_asics = conseguir_informacoes_processos(links_empresa_asics, driver)\n",
    "salvar_arquivo_csv('ds_asics.csv', info_processos_asics)\n",
    "\n",
    "links_empresa_under_armour = conseguir_links_empresa('https://www.jusbrasil.com.br/consulta-processual/busca?q=Under+Armour+Brasil+Com%C3%A9rcio+e+Distribui%C3%A7%C3%A3o+de+Artigos+Esportivos+Ltda', driver)\n",
    "info_processos_under_armour = conseguir_informacoes_processos(links_empresa_under_armour, driver)\n",
    "salvar_arquivo_csv('ds_under_armour.csv', info_processos_under_armour)\n",
    "\n",
    "links_empresa_reebok = conseguir_links_empresa('https://www.jusbrasil.com.br/consulta-processual/busca?q=reebok+produtos+esportivos+ltda.', driver)\n",
    "info_processos_rebook = conseguir_informacoes_processos(links_empresa_reebok, driver)\n",
    "salvar_arquivo_csv('ds_rebook.csv', info_processos_rebook)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60da999-d3ab-43d7-9388-670663b7ae37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bbe227-dfb9-4842-9523-55370b2ed708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
